# Example city.toml for Kubernetes deployment.
#
# Two deployment modes:
#
# A) Local controller (demo mode):
#    1. Apply K8s manifests: kubectl apply -f contrib/k8s/
#    2. Build agent image: make docker-base docker-agent
#    3. Set environment:
#       export GC_K8S_IMAGE=gc-agent:latest
#    4. gc start --foreground <city-path>
#
# B) In-cluster controller (production):
#    1. Apply K8s manifests + controller RBAC:
#       kubectl apply -f contrib/k8s/
#       kubectl apply -f contrib/k8s/controller-rbac.yaml
#    2. Build images: make docker-base docker-agent docker-controller
#    3. contrib/session-scripts/gc-controller-k8s deploy <city-path>

[workspace]
name = "my-k8s-city"
provider = "claude"
session_template = "{{.City}}-{{.Agent}}"
# No worktree isolation in K8s Phase 1 — repo is cloned into the image.
isolation = "none"

# Native K8s session provider — uses client-go for direct API calls.
# Eliminates subprocess overhead vs the exec-based gc-session-k8s script.
# Pod manifests are compatible with gc-session-k8s for mixed-mode migration.
[session]
provider = "k8s"

# K8s-specific settings. Env vars (GC_K8S_*) override TOML values.
[session.k8s]
namespace = "gc"
# image = "gc-agent:latest"       # or set GC_K8S_IMAGE env var
# context = ""                     # uses current kubeconfig context
# cpu_request = "500m"
# mem_request = "1Gi"
# cpu_limit = "2"
# mem_limit = "4Gi"

[mail]
provider = "exec:gc-mail-mcp-agent-mail"

# For 50+ agents, increase patrol_interval to reduce API pressure.
# Default is 30s. Increase further for 100+ agents.
# [daemon]
# patrol_interval = "30s"

[[agents]]
name = "mayor"
prompt_template = "prompts/mayor.md.tmpl"
install_hooks = ["claude"]
process_names = ["claude"]
ready_prompt_prefix = "claude"
nudge = "Check mail and hook status, then act accordingly."

[[agents]]
name = "coder"
prompt_template = "prompts/coder.md.tmpl"
install_hooks = ["claude"]
process_names = ["claude"]
ready_prompt_prefix = "claude"
nudge = "Check your hook for work assignments."
pool = 2
