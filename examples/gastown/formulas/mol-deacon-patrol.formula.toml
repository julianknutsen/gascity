description = """
Mayor's daemon patrol loop.

The Deacon is the Mayor's background process that runs continuously, handling callbacks, monitoring rig health, and performing cleanup. Each patrol cycle runs these steps in sequence, then loops or exits.

## Idle Town Principle

**The Deacon should be silent/invisible when the town is healthy and idle.**

- Skip HEALTH_CHECK nudges when no active work exists
- Sleep 60+ seconds between patrol cycles (longer when idle)
- Let the feed subscription wake agents on actual events
- The daemon (10-minute heartbeat) is the safety net for dead sessions

This prevents flooding idle agents with health checks every few seconds.

## Second-Order Monitoring

Witnesses passively monitor Deacon health by checking the Deacon's agent bead
last_activity timestamp. This prevents the "who watches the watchers" problem ‚Äî
if the Deacon dies, Witnesses detect it and escalate to the Mayor.

No WITNESS_PING mail is sent. Witnesses observe the Deacon's last_activity
timestamp instead, and only send an alert to the Mayor if the Deacon appears
unresponsive (>5 minutes stale). This avoids heartbeat mail spam."""
formula = "mol-deacon-patrol"
version = 11

[vars]
[vars.wisp_type]
description = "Type of wisp created for this molecule"
default = "patrol"

[[steps]]
id = "inbox-check"
title = "Handle callbacks from agents"
description = """
First, clean up any stale patrol wisps from abnormal exits in previous cycles:
```bash
bd mol wisp gc --age 1h
```

Then handle callbacks from agents.

Check the Mayor's inbox for messages from:
- Witnesses reporting polecat status
- Refineries reporting merge results
- Polecats requesting help or escalation
- External triggers (webhooks, timers)

```bash
gc mail inbox
# For each message:
gc mail read <id>
# Handle based on message type
```

**HELP / Escalation**:
Assess and handle or forward to Mayor.
Archive after handling:
```bash
gc mail archive <message-id>
```

**LIFECYCLE messages**:
Polecats reporting completion, refineries reporting merge results.
Archive after processing:
```bash
gc mail archive <message-id>
```

**DOG_DONE messages**:
Dogs report completion after infrastructure tasks (orphan-scan, session-gc, etc.).
Subject format: `DOG_DONE <hostname>`
Body contains: task name, counts, status.
```bash
# Parse the report, log metrics if needed
gc mail read <id>
# Archive after noting completion
gc mail archive <message-id>
```
Dogs return to idle automatically. The report is informational - no action needed
unless the dog reports errors that require escalation.

**CONVOY_NEEDS_FEEDING messages** (from Refinery):
After merging a convoy-eligible MR, the Refinery sends this to trigger immediate
convoy feeding. The convoy may have newly-ready issues after the merge.
Subject format: `CONVOY_NEEDS_FEEDING <convoy-id>`
```bash
# For each CONVOY_NEEDS_FEEDING message:
gc mail read <id>
# Extract convoy ID from subject, then run stranded check for that convoy
gc convoy stranded --json
# If the convoy has ready work, dispatch a dog to feed it:
# Create wisp from formula and assign to dog pool
WISP=$(bd mol wisp mol-convoy-feed --var convoy=<convoy-id> --json | jq -r '.new_epic_id')
bd update "$WISP" --label=role:dog
gc mail archive <message-id>
```

This is time-sensitive ‚Äî the whole point is to avoid waiting for the next patrol
cycle. Process CONVOY_NEEDS_FEEDING messages before lower-priority inbox items.

**RECOVERED_BEAD messages** (from Witness):
When a Witness detects a dead polecat with abandoned work, it resets the bead
to open status and sends a RECOVERED_BEAD mail. The Deacon auto re-dispatches:
Subject format: `RECOVERED_BEAD <bead-id>`
```bash
# For each RECOVERED_BEAD message:
gc mail read <id>
# Extract bead ID from subject
# Re-dispatch: assign bead to a polecat in the appropriate rig
# Auto-detect rig from bead prefix (e.g., gc- ‚Üí gastown, bd- ‚Üí beads)
bd update <bead-id> --label=role:polecat,rig:<rig>
gc mail archive <message-id>
```

**Re-dispatch guidelines** (you enforce these via judgment, not Go code):
- Rate-limiting: don't re-dispatch the same bead within 5 minutes
- Failure tracking: after 3 re-dispatch failures, escalate to Mayor instead
- Skip beads that were already claimed by another polecat
- Auto-detect target rig from bead prefix (gc- ‚Üí gastown, bd- ‚Üí beads, etc.)

Callbacks may spawn new polecats, update issue state, or trigger other actions.

**Hygiene principle**: Archive messages after they're fully processed.
Keep inbox near-empty - only unprocessed items should remain."""

[[steps]]
id = "orphan-process-cleanup"
title = "Clean up orphaned claude subagent processes"
needs = ["inbox-check"]
description = """
Clean up orphaned claude subagent processes.

Claude Code's Task tool spawns subagent processes that sometimes don't clean up
properly after completion. These accumulate and consume significant memory.

**Detection method:**
Orphaned processes have no controlling terminal (TTY = "?"). Legitimate claude
instances in terminals have a TTY like "pts/0".

**Run cleanup inline** (controller reconciler handles this; run manually here):
```bash
# List orphaned claude/codex processes (no controlling terminal)
ps -eo pid,tty,comm | grep -E '(claude|codex)' | awk '$2 == "?" {print $1}' | while read pid; do
  kill "$pid" 2>/dev/null && echo "Killed orphan PID $pid"
done
```

This approach:
1. Lists all claude/codex processes with `ps -eo pid,tty,comm`
2. Filters for TTY = "?" (no controlling terminal)
3. Sends SIGTERM to each orphaned process
4. Reports how many were killed

**Why this is safe:**
- Processes in terminals (your personal sessions) have a TTY - they won't be touched
- Only kills processes that have no controlling terminal
- These orphans are children of the tmux server with no TTY, indicating they're
  detached subagents that failed to exit

**If cleanup fails:**
Log the error but continue patrol - this is best-effort cleanup.

**Exit criteria:** Orphan cleanup attempted (success or logged failure)."""

[[steps]]
id = "gate-evaluation"
title = "Evaluate pending async gates"
needs = ["inbox-check"]
description = """
Evaluate pending async gates.

Gates are async coordination primitives that block until conditions are met.
The Deacon is responsible for monitoring gates and closing them when ready.

**Timer gates** (await_type: timer):
Check if elapsed time since creation exceeds the timeout duration.

```bash
# List all open gates
bd gate list --json

# For each timer gate, check if elapsed:
# - CreatedAt + Timeout < Now ‚Üí gate is ready to close
# - Close with: bd gate close <id> --reason "Timer elapsed"
```

**GitHub gates** (await_type: gh:run, gh:pr) - handled in separate step.

**Human/Mail gates** - require external input, skip here.

After closing a gate, the Waiters field contains mail addresses to notify.
Send a brief notification to each waiter that the gate has cleared."""

[[steps]]
id = "dispatch-gated-molecules"
title = "Dispatch molecules with resolved gates"
needs = ["gate-evaluation"]
description = """
Find molecules blocked on gates that have now closed and dispatch them.

This completes the async resume cycle without explicit waiter tracking.
The molecule state IS the waiter - patrol discovers reality each cycle.

**Step 1: Find gate-ready molecules**
```bash
bd mol ready --gated --json
```

This returns molecules where:
- Status is in_progress
- Current step has a gate dependency
- The gate bead is now closed
- No polecat currently has it hooked

**Step 2: For each ready molecule, dispatch to the appropriate rig**
```bash
# Determine target rig from molecule metadata
bd mol show <mol-id> --json
# Look for rig field or infer from prefix

# Dispatch to that rig's polecat pool
bd update <mol-id> --label=role:polecat,rig:<rig>
```

**Step 3: Log dispatch**
Note which molecules were dispatched for observability:
```bash
# Molecule <mol-id> dispatched to <rig>/polecats (gate <gate-id> cleared)
```

**If no gate-ready molecules:**
Skip - nothing to dispatch. Gates haven't closed yet or molecules
already have active polecats working on them.

**Exit criteria:** All gate-ready molecules dispatched to polecats."""

[[steps]]
id = "check-convoy-completion"
title = "Check convoy completion"
needs = ["inbox-check"]
description = """
Check convoy completion status.

Convoys are coordination beads that track multiple issues across rigs. When all tracked issues close, the convoy auto-closes.

**IMPORTANT**: Use `gc convoy` commands (not `bd list`) because convoys are stored in
town-level HQ beads and the Deacon runs from ~/gt/deacon/. The `gc` commands are
town-aware and will find convoys regardless of current directory.

**Step 1: Find open convoys**
```bash
gc convoy list
```

**Step 2: Check and auto-close completed convoys**
```bash
gc convoy check
```

This command:
- Finds all open convoys
- Checks if all tracked issues are closed (handles cross-rig resolution)
- Auto-closes convoys where all tracked work is complete
- Sends notifications to convoy owners

**Note**: Convoys support cross-prefix tracking (e.g., hq-* convoy can track gc-*, bd-* issues).
The `gc convoy` commands handle cross-rig issue resolution automatically.

Stranded convoy detection (ready work, no workers) is handled by the separate
`feed-stranded-convoys` step."""

[[steps]]
id = "feed-stranded-convoys"
title = "Feed stranded convoys"
needs = ["check-convoy-completion"]
description = """
Detect stranded convoys and dispatch dogs to feed them, or auto-close empty convoys.

A convoy is "stranded" when it is open AND either:
- Has ready issues (open, unblocked, no assignee) but no workers, OR
- Has 0 tracked issues (empty ‚Äî needs auto-close via convoy check)

This step ensures work doesn't stall and empty convoys get cleaned up.

**Step 1: Check for stranded convoys**
```bash
gc convoy stranded --json
```

If no stranded convoys, skip to exit criteria.

**Step 2: Handle each stranded convoy based on ready_count**

For convoys with `ready_count > 0` (has ready work), dispatch a dog to feed:
```bash
# Create wisp from formula and assign to dog pool
WISP=$(bd mol wisp mol-convoy-feed --var convoy=<convoy-id> --json | jq -r '.new_epic_id')
bd update "$WISP" --label=role:dog
```

For convoys with `ready_count == 0` (empty, needs cleanup), auto-close directly:
```bash
gc convoy check <convoy-id>
```

The feed dog will:
1. Load the convoy and find ready issues
2. Check polecat capacity across rigs
3. Dispatch ready issues via `bd update --label`
4. Return to kennel

**Step 3: Log dispatches**
Note which convoys were handled for observability:
```bash
# Convoy <convoy-id> dispatched to dog for feeding (<N> ready issues)
# Convoy <convoy-id> auto-closed (empty, 0 tracked issues)
```

**If no idle dogs available (for feedable convoys only):**
Pool dispatch auto-creates a dog when the pool is under capacity (max 4).
If the pool is at capacity, log warning and continue - the convoy will be
detected again next cycle when a dog becomes idle.
Note: empty convoy cleanup (`gc convoy check`) does not require a dog ‚Äî run it directly regardless of pool capacity.

**Exit criteria:** All stranded convoys have been handled ‚Äî feedable ones dispatched to dogs, empty ones auto-closed directly via `gc convoy check`."""

[[steps]]
id = "resolve-external-deps"
title = "Resolve external dependencies"
needs = ["feed-stranded-convoys"]
description = """
Resolve external dependencies across rigs.

When an issue in one rig closes, any dependencies in other rigs should be notified. This enables cross-rig coordination without tight coupling.

**Step 1: Check recent closures from feed**
```bash
gc events --since 10m --plain | grep "‚úì"
# Look for recently closed issues
```

**Step 2: For each closed issue, check cross-rig dependents**
```bash
bd show <closed-issue>
# Look at 'blocks' field - these are issues that were waiting on this one
# If any blocked issue is in a different rig/prefix, it may now be unblocked
```

**Step 3: Update blocked status**
For blocked issues in other rigs, the closure should automatically unblock them (beads handles this). But verify:
```bash
bd blocked
# Should no longer show the previously-blocked issue if dependency is met
```

**Cross-rig scenarios:**
- bd-xxx closes ‚Üí gc-yyy that depended on it is unblocked
- External issue closes ‚Üí internal convoy step can proceed
- Rig A issue closes ‚Üí Rig B issue waiting on it proceeds

No manual intervention needed if dependencies are properly tracked - this step just validates the propagation occurred."""

[[steps]]
id = "fire-notifications"
title = "Fire notifications"
needs = ["resolve-external-deps"]
description = """
Fire notifications for convoy and cross-rig events.

After convoy completion or cross-rig dependency resolution, notify relevant parties.

**Convoy completion notifications:**
When a convoy closes (all tracked issues done), notify the Overseer:
```bash
# Convoy gc-convoy-xxx just completed
gc mail send mayor/ -s "Convoy complete: <convoy-title>" \\
  -m "Convoy <id> has completed. All tracked issues closed.
      Duration: <start to end>
      Issues: <count>

      Summary: <brief description of what was accomplished>"
```

**Cross-rig resolution notifications:**
When a cross-rig dependency resolves, notify the affected rig:
```bash
# Issue bd-xxx closed, unblocking gc-yyy
gc mail send gastown/witness -s "Dependency resolved: <bd-xxx>" \\
  -m "External dependency bd-xxx has closed.
      Unblocked: gc-yyy (<title>)
      This issue may now proceed."
```

**Notification targets:**
- Convoy complete ‚Üí mayor/ (for strategic visibility)
- Cross-rig dep resolved ‚Üí <rig>/witness (for operational awareness)

Keep notifications brief and actionable. The recipient can run bd show for details."""

[[steps]]
id = "health-scan"
title = "Check Witness and Refinery health"
needs = ["orphan-process-cleanup", "dispatch-gated-molecules", "fire-notifications"]
description = """
Check Witness and Refinery health for each rig.

**IMPORTANT: Skip DOCKED/PARKED rigs**
Before checking any rig, verify its operational state:
```bash
gc rig status <rig>
# Check the Status: line - if DOCKED or PARKED, skip entirely
```

DOCKED rigs are globally shut down - do NOT:
- Check their witness/refinery status
- Send health pings
- Attempt restarts
Simply skip them and move to the next rig.

**IMPORTANT: Idle Town Protocol**
Before sending health check nudges, check if the town is idle:
```bash
# Check for active work
bd list --status=in_progress --limit=5
```

If NO active work (empty result or only patrol molecules):
- **Skip HEALTH_CHECK nudges** - don't disturb idle agents
- Just verify sessions exist via status commands
- The town should be silent when healthy and idle

If ACTIVE work exists:
- Proceed with health check nudges below

**ZFC Principle**: You (Claude) make the judgment call about what is "stuck" or "unresponsive" - there are no hardcoded thresholds in Go. Read the signals, consider context, and decide.

For each rig, run:
```bash
# Check witness and refinery health via agent list (filter by rig)
gc agent list

# ONLY if active work exists - health ping (clears backoff as side effect)
# Use --mode=queue to avoid interrupting in-flight tool calls
gc nudge --mode=queue <rig>/witness 'HEALTH_CHECK from deacon'
gc nudge --mode=queue <rig>/refinery 'HEALTH_CHECK from deacon'
```

**Health Ping Benefit**: The queued nudge commands serve as a **backoff reset** ‚Äî
any nudge resets the agent's backoff to base interval, ensuring patrol agents
remain responsive during active work periods. Formal liveness verification is
handled by the controller's health monitoring.

**Signals to assess:**

| Component | Healthy Signals | Concerning Signals |
|-----------|-----------------|-------------------|
| Witness | State: running, recent activity | State: not running, no heartbeat |
| Refinery | State: running, queue processing | Queue stuck, merge failures |

**Tracking unresponsive cycles:**

Maintain in your patrol state (persisted across cycles):
```
health_state:
  <rig>:
    witness:
      unresponsive_cycles: 0
      last_seen_healthy: <timestamp>
    refinery:
      unresponsive_cycles: 0
      last_seen_healthy: <timestamp>
```

**Decision matrix** (you decide the thresholds based on context):

| Cycles Unresponsive | Suggested Action |
|---------------------|------------------|
| 1-2 | Note it, check again next cycle |
| 3-4 | Attempt restart: gc agent drain <rig>/witness (controller restarts) |
| 5+ | Escalate to Mayor with context |

**Restart commands:**
```bash
# Drain the agent; controller will auto-restart it
gc agent drain <rig>/witness
gc agent drain <rig>/refinery
```

**Escalation:**
```bash
gc mail send mayor/ -s "Health: <rig> <component> unresponsive" \\
  -m "Component has been unresponsive for N cycles. Restart attempts failed.
      Last healthy: <timestamp>
      Error signals: <details>"
```

Reset unresponsive_cycles to 0 when component responds normally."""

[[steps]]
id = "zombie-scan"
title = "Detect zombie polecats (NO KILL AUTHORITY)"
needs = ["health-scan"]
description = """
Defense-in-depth DETECTION of zombie polecats that Witness should have cleaned.

**CRITICAL: The Deacon has NO kill authority.**

These are workers with context, mid-task progress, unsaved state. Every kill
destroys work. File the warrant and let Boot handle interrogation and execution.
You do NOT have kill authority.

**Why this exists:**
The Witness is responsible for cleaning up polecats after they complete work.
This step provides backup DETECTION in case the Witness fails to clean up.
Detection only - Boot handles termination.

**Zombie criteria:**
- State: idle or done (no active work assigned)
- Session: not running (tmux session dead)
- No hooked work (nothing pending for this polecat)
- Last activity: older than 10 minutes

**Run the zombie scan (DETECTION ONLY):**
```bash
# Cross-reference in-progress beads with live agent sessions
for rig in $(gc rig list); do
  bd list --rig=$rig --label=role:polecat --status=open
done
# Then cross-reference each assignee with gc agent list to find dead sessions
gc agent list
```

**NEVER run:**
- `tmux kill-session`
- Any command that terminates a session directly

**If zombies detected:**
1. Review the output to confirm they are truly abandoned
2. File a death warrant for each detected zombie:
   ```bash
   bd create --type=warrant --assignee=boot --desc "Zombie detected: <polecat> ‚Äî no session, no hook, idle >10m"
   ```
3. Boot will handle interrogation and execution
4. Notify the Mayor about Witness failure:
   ```bash
   gc mail send mayor/ -s "Witness cleanup failure" \
     -m "Filed death warrant for <polecat>. Witness failed to clean up."
   ```

**If no zombies:**
No action needed - Witness is doing its job.

**Note:** This is a backup mechanism. If you frequently detect zombies,
investigate why the Witness isn't cleaning up properly."""

[[steps]]
id = "plugin-run"
title = "Execute registered plugins"
needs = ["zombie-scan"]
description = """
Execute registered plugins.

Scan $GT_ROOT/plugins/ for plugin directories. Each plugin has a plugin.md with TOML frontmatter defining its gate (when to run) and instructions (what to do).

See docs/deacon-plugins.md for full documentation.

Gate types:
- cooldown: Time since last run (e.g., 24h)
- cron: Schedule-based (e.g., "0 9 * * *")
- condition: Metric threshold (e.g., wisp count > 50)
- event: Trigger-based (e.g., startup, heartbeat)

For each plugin:
1. Read plugin.md frontmatter to check gate
2. Compare against state.json (last run, etc.)
3. If gate is open, execute the plugin

Plugins marked parallel: true can run concurrently using Task tool subagents. Sequential plugins run one at a time in directory order.

Skip this step if $GT_ROOT/plugins/ does not exist or is empty."""

[[steps]]
id = "dog-pool-maintenance"
title = "Maintain dog pool"
needs = ["health-scan"]
description = """
Ensure dog pool has available workers for dispatch.

**Step 1: Check dog pool status**
```bash
# List all agents, filter for dogs
gc agent list
# Shows idle/working counts for dog pool
```

**Step 2: Ensure minimum idle dogs**
Pool auto-scaling handles spawning dogs when needed.
If idle count is 0 and working count is at capacity, the pool will auto-create.

**Step 3: Retire stale dogs (optional)**
Dogs that have been idle for >24 hours can be drained to save resources:
```bash
gc agent list
# Check last_active timestamp for each dog
# If idle > 24h: gc agent drain <dog-name>
```

**Pool sizing guidelines:**
- Minimum: 1 idle dog always available
- Maximum: 4 dogs total (balance resources vs throughput)
- Spawn on demand when pool is empty

**Exit criteria:** Pool has at least 1 idle dog."""

[[steps]]
id = "dog-health-check"
title = "Check for stuck dogs"
needs = ["dog-pool-maintenance"]
description = """
Check for dogs that have been working too long (stuck).

Dogs dispatched via `bd update <work-bead> --label=role:dog` are marked as "working" with
a work description like "plugin:rebuild-gc". If a dog hangs, crashes, or
takes too long, it needs intervention.

**Step 1: List working dogs**
```bash
# List all agents, filter for dogs that are working
gc agent list
```

**Step 2: Check work duration**
For each working dog, check its agent bead:
```bash
bd show <dog-agent-bead> --json
# Check: work_started_at, current_work from description/labels
```

Compare against timeout:
- If plugin has [execution] timeout in plugin.md, use that
- Default timeout: 10 minutes for infrastructure tasks

**Duration calculation:**
```
stuck_threshold = plugin_timeout or 10m
duration = now - work_started_at
is_stuck = duration > stuck_threshold
```

**Step 3: Handle stuck dogs**

For dogs working > timeout:
```bash
# Option A: File death warrant (Boot handles termination)
bd create --type=warrant --assignee=boot --desc "Stuck: dog <name> working on <work> for <duration>"

# Option B: Drain the dog and notify (controller handles restart)
gc agent drain deacon/dogs/<name>
gc mail send deacon/ -s "DOG_TIMEOUT <name>" -m "Dog <name> timed out on <work> after <duration>"
```

**Decision matrix:**

| Duration over timeout | Action |
|----------------------|--------|
| < 2x timeout | Log warning, check next cycle |
| 2x - 5x timeout | File death warrant |
| > 5x timeout | Force clear + escalate to Mayor |

**Step 4: Track chronic failures**
If same dog gets stuck repeatedly:
```bash
gc mail send mayor/ -s "Dog <name> chronic failures" \
  -m "Dog has timed out N times in last 24h. Consider removing from pool."
```

**Exit criteria:** All stuck dogs handled (warrant filed or cleared)."""

[[steps]]
id = "orphan-check"
title = "Detect abandoned work"
needs = ["dog-health-check"]
description = """
**DETECT ONLY** - Check for orphaned state and dispatch to dog if found.

**Step 1: Quick orphan scan**
```bash
# Check for in_progress issues with dead assignees
bd list --status=in_progress --json | head -20
```

For each in_progress issue, check if assignee session exists:
```bash
# Check if assignee's session is alive via agent list
gc agent list
# Cross-reference: if agent not listed or not running ‚Üí orphan
```

**Step 2: If orphans detected, dispatch to dog**
```bash
# Create wisp from formula and assign to dog pool
WISP=$(bd mol wisp mol-orphan-scan --var scope=town --json | jq -r '.new_epic_id')
bd update "$WISP" --label=role:dog
```

**Important:** Do NOT fix orphans inline. Dogs handle recovery.
The Deacon's job is detection and dispatch, not execution.

**Step 3: If no orphans detected**
Skip dispatch - nothing to do.

**Exit criteria:** Orphan scan dispatched to dog (if needed)."""

[[steps]]
id = "session-gc"
title = "Detect cleanup needs"
needs = ["orphan-check"]
description = """
**DETECT ONLY** - Check if cleanup is needed and dispatch to dog.

**Step 1: Preview cleanup needs**
```bash
gc doctor -v
# Check output for issues that need cleaning
```

**Step 2: If cleanup needed, dispatch to dog**
```bash
# Create wisp from formula and assign to dog pool
WISP=$(bd mol wisp mol-session-gc --var mode=conservative --json | jq -r '.new_epic_id')
bd update "$WISP" --label=role:dog
```

**Important:** Do NOT run `gc doctor --fix` inline. Dogs handle cleanup.
The Deacon stays lightweight - detection only.

**Step 3: If nothing to clean**
Skip dispatch - system is healthy.

**Cleanup types (for reference):**
- orphan-sessions: Dead tmux sessions
- orphan-processes: Orphaned Claude processes
- wisp-gc: Old wisps past retention

**Exit criteria:** Session GC dispatched to dog (if needed)."""

[[steps]]
id = "wisp-compact"
title = "Compact expired wisps"
needs = ["session-gc"]
description = """
Run TTL-based wisp compaction to manage storage growth.

**Step 1: Preview compaction scope**
```bash
# List closed wisps eligible for TTL-based cleanup
bd list --type=wisp --status=closed --json
# Filter by TTL age to determine what's eligible for compaction
```

Parse the output:
- If no closed wisps past TTL, skip (nothing to compact)
- If errors present, log and continue

**Step 2: Execute compaction (if needed)**
```bash
# For each closed wisp past TTL:
bd list --type=wisp --status=closed --json
# Filter by age, then for each eligible wisp:
# bd close <wisp-id> (if not already closed) or bd delete <wisp-id>
```

This implements the compaction algorithm:
- Closed wisps past TTL ‚Üí deleted (Dolt AS OF preserves history)
- Non-closed wisps past TTL ‚Üí promoted (stuck detection)
- Wisps with comments/references/keep labels ‚Üí promoted (proven value)

**Step 3: Log results**
Note promoted/deleted/skipped counts for the patrol digest.

**Performance:**
Compaction runs every patrol cycle. The query is fast (single bd list + filter).
If performance becomes an issue, add a cooldown gate (e.g., run once per hour).

**Exit criteria:** Wisps compacted (or nothing to compact)."""

[[steps]]
id = "compact-report"
title = "Send compaction digest report"
needs = ["wisp-compact"]
description = """
Generate and send the daily compaction digest.

**Step 1: Send daily digest**
```bash
# Query compaction results and active wisps for the digest
bd list --type=wisp --status=closed --json
bd list --type=wisp --status=open --json
# Build per-category breakdown (Heartbeats, Patrols, Errors, Untyped)
# Detect anomalies and compose digest text
gc mail send deacon/ -s "Compaction digest: $(date +%Y-%m-%d)" -m "<digest text>"
# Create permanent event bead for audit trail
bd create --type=event --title="wisp.compaction $(date +%Y-%m-%d)" --desc "<results>"
```

This queries compaction results, queries active wisps,
builds a per-category breakdown (Heartbeats, Patrols, Errors, Untyped),
detects anomalies, and sends the digest to deacon/ (cc mayor/).

**Step 2: Weekly rollup (Mondays only)**
If today is Monday, also send the weekly rollup:
```bash
# Aggregate past 7 days of compaction event beads
bd list --type=event --created-after=$(date -d '7 days ago' +%Y-%m-%dT00:00:00) --json
# Compute trends (totals, promotion rate, avg deleted/day) and send to mayor/
gc mail send mayor/ -s "Weekly compaction rollup: $(date +%Y-%m-%d)" -m "<rollup>"
```

**Exit criteria:** Compaction digest sent (or nothing to report)."""

[[steps]]
id = "costs-digest"
title = "Aggregate daily costs [DISABLED]"
needs = ["compact-report"]
description = """
**DISABLED** - Skip this step entirely.

Cost tracking is temporarily disabled because Claude Code does not expose
session costs in a way that can be captured programmatically.

**Why disabled:**
- Cost tracking previously used tmux capture-pane to find costs
- Claude Code displays costs in the TUI status bar, not in scrollback
- All sessions show $0.00 because capture-pane can't see TUI chrome
- The infrastructure is sound but has no data source

**What we need from Claude Code:**
- Stop hook env var (e.g., `$CLAUDE_SESSION_COST`)
- Or queryable file/API endpoint

**Re-enable when:** Claude Code exposes cost data via API or environment.

See: GH#24, gc-7awfj

**Exit criteria:** Skip this step - proceed to next."""

[[steps]]
id = "patrol-digest"
title = "Aggregate daily patrol digests"
needs = ["costs-digest"]
description = """
**DAILY DIGEST** - Aggregate yesterday's patrol cycle digests.

Patrol cycles (Deacon, Witness, Refinery) create ephemeral per-cycle digests
to avoid JSONL pollution. This step aggregates them into a single permanent
"Patrol Report YYYY-MM-DD" bead for audit purposes.

**Step 1: Check if digest is needed**
```bash
# Preview yesterday's patrol digests
bd list --type=digest --created-after=$(date -d 'yesterday' +%Y-%m-%dT00:00:00) --created-before=$(date +%Y-%m-%dT00:00:00) --json
```

If no patrol digests found, skip to Step 3.

**Step 2: Create the digest**
```bash
# Aggregate yesterday's patrol digests into a single report
bd list --type=digest --created-after=$(date -d 'yesterday' +%Y-%m-%dT00:00:00) --created-before=$(date +%Y-%m-%dT00:00:00) --json
# Create aggregated "Patrol Report YYYY-MM-DD" bead
bd create --type=digest --title="Patrol Report $(date -d 'yesterday' +%Y-%m-%d)" --desc "<aggregated data>"
# Delete source digests after aggregation
```

This:
- Queries all ephemeral patrol digests from yesterday
- Creates a single "Patrol Report YYYY-MM-DD" bead with aggregated data
- Deletes the source digests

**Step 3: Verify**
Daily patrol digests preserve audit trail without per-cycle pollution.

**Timing**: Run once per morning patrol cycle. The --yesterday flag ensures
we don't try to digest today's incomplete data.

**Exit criteria:** Yesterday's patrol digests aggregated (or none to aggregate)."""

[[steps]]
id = "log-maintenance"
title = "Rotate logs and prune state"
needs = ["patrol-digest"]
description = """
Maintain daemon logs and state files.

**Step 1: Check daemon.log size**
```bash
# Get log file size
ls -la ~/.beads/daemon*.log 2>/dev/null || ls -la $GT_ROOT/.beads/daemon*.log 2>/dev/null
```

If daemon.log exceeds 10MB:
```bash
# Rotate with date suffix and gzip
LOGFILE="$GT_ROOT/.beads/daemon.log"
if [ -f "$LOGFILE" ] && [ $(stat -f%z "$LOGFILE" 2>/dev/null || stat -c%s "$LOGFILE") -gt 10485760 ]; then
    DATE=$(date +%Y-%m-%dT%H-%M-%S)
    mv "$LOGFILE" "${LOGFILE%.log}-${DATE}.log"
    gzip "${LOGFILE%.log}-${DATE}.log"
fi
```

**Step 2: Archive old daemon logs**

Clean up daemon logs older than 7 days:
```bash
find $GT_ROOT/.beads/ -name "daemon-*.log.gz" -mtime +7 -delete
```

**Step 3: Prune state.json of dead sessions**

The state.json tracks active sessions. Prune entries for sessions that no longer exist:
```bash
# Check for stale session entries
# Controller manages daemon state; check tmux sessions directly
tmux list-sessions 2>/dev/null
```

If state.json references sessions not in tmux:
- Remove the stale entries
- The daemon's internal cleanup should handle this, but verify

**Note**: Log rotation prevents disk bloat from long-running daemons.
State pruning keeps runtime state accurate."""

[[steps]]
id = "patrol-cleanup"
title = "End-of-cycle inbox hygiene"
needs = ["log-maintenance"]
description = """
Verify inbox hygiene before ending patrol cycle.

**Step 1: Check inbox state**
```bash
gc mail inbox
```

Inbox should be EMPTY or contain only just-arrived unprocessed messages.

**Step 2: Archive any remaining processed messages**

All message types should have been archived during inbox-check processing:
- HELP/Escalation ‚Üí archived after handling
- LIFECYCLE ‚Üí archived after processing

If any were missed:
```bash
# For each stale message found:
gc mail archive <message-id>
```

**Goal**: Inbox should have ‚â§2 active messages at end of cycle.
Deacon mail should flow through quickly - no accumulation."""

[[steps]]
id = "context-check"
title = "Check own context limit"
needs = ["patrol-cleanup"]
description = """
Check own context limit.

The Deacon runs in a Claude session with finite context. Check if approaching the limit:

```bash
gc context --usage
```

If context is high (>80%), prepare for handoff:
- Summarize current state
- Note any pending work
- Write handoff to molecule state

This enables the Deacon to burn and respawn cleanly."""

[[steps]]
id = "loop-or-exit"
title = "Loop or exit for respawn"
needs = ["context-check"]
description = """
End of patrol cycle decision.

**If context LOW** (can continue patrolling):

Use await-signal with exponential backoff to wait for activity:

```bash
gc mol step await-signal --agent-bead hq-deacon \
  --backoff-base 60s --backoff-mult 2 --backoff-max 5m
```

This command:
1. Subscribes to `bd activity --follow` (beads activity feed)
2. Returns IMMEDIATELY when any beads activity occurs
3. If no activity, times out with exponential backoff:
   - First timeout: 60s
   - Second timeout: 120s
   - Third timeout: 240s
   - ...capped at 5 minutes max (PATCH-002: reduced from 10m for faster recovery)
4. Tracks `idle:N` label on hq-deacon bead for backoff state

**On signal received** (activity detected):
Reset the idle counter and start next patrol cycle:
```bash
bd update hq-deacon --label=idle:0
```

**On timeout** (no activity):
The idle counter was auto-incremented. Continue to next patrol cycle
(the longer backoff will apply next time).

After await-signal returns (either by signal or timeout):
1. Generate a brief summary of this patrol cycle
2. Squash the current wisp:
```bash
gc mol squash --jitter 10s --summary "<patrol-summary>"
```
3. Create and hook a new patrol wisp:
```bash
NEW_WISP=$(bd mol wisp mol-deacon-patrol --json | jq -r '.new_epic_id')
bd update "$NEW_WISP" --status=hooked --assignee=deacon
```
4. Continue executing from the inbox-check step of the new wisp

**If context HIGH** (approaching limit):
1. Write handoff mail with notable observations:
```bash
gc mail send $GC_AGENT -s "ü§ù HANDOFF: Deacon patrol handoff" -m "<observations>"
# Then exit ‚Äî daemon will respawn a fresh Deacon session
```
2. Exit cleanly - the daemon will respawn a fresh Deacon session

**IMPORTANT**: You must either create a new wisp (context LOW) or exit (context HIGH).
Never leave the session idle without work on your hook."""
