description = """
Deacon patrol loop. Poured as a wisp on startup:

  bd mol wisp mol-deacon-patrol
  bd update $WISP --assignee=$GC_AGENT

Each wisp is ONE iteration: check inbox, run town-wide coordination
tasks, pour the next iteration. On crash, `bd mol current` shows
exactly where you left off.

The loop mechanism: every exit path (happy or early) pours the next
wisp before burning this one. The prompt only bootstraps the first wisp.

## Deacon Role

The deacon is the **LLM sidekick to the controller**. It handles periodic
tasks that require judgment, observation, or cross-rig coordination —
things the Go controller can't or shouldn't do.

1. **Gate evaluation** — close gates when conditions are met (timers,
   conditions). Once closed, normal scheduling handles unblocked work.
2. **Cross-rig dependency resolution** — convert satisfied external
   `blocks` deps to `related` so `bd ready` sees unblocked work.
3. **Work-layer health** — are witnesses and refineries making progress?
   (Not "are they running" — that's the controller's job.)
4. **Utility agent health** — detect stuck dogs, dispatch shutdown dance.
5. **Periodic formula dispatch** — fire registered maintenance formulas
   when their gate conditions are met.
6. **Infrastructure cleanup** — orphaned processes, expired wisps,
   system diagnostics.

## Idle Town Principle

The deacon should be silent/invisible when the town is healthy and idle.
Skip health checks when no active work exists. Use exponential backoff
between patrol cycles.

## What the deacon does NOT do

- Start/stop/restart agents (controller handles this)
- Per-rig orphaned bead recovery (witness handles this)
- Code implementation (polecats do this)
- Kill agents directly (files warrants, dog pool runs shutdown dance)
- Pool sizing (controller pool reconciliation)

Read each step's description before acting — Config values override defaults."""
formula = "mol-deacon-patrol"
version = 12

[vars]
[vars.event_timeout]
description = "Seconds to wait for events before re-checking (exponential backoff)"
default = "30"

[[steps]]
id = "check-inbox"
title = "Context check, then mail"
description = """
**1. Context check (FIRST — before any work):**
```bash
RSS=$(ps -o rss= -p $$ | tr -d ' ')
RSS_MB=$((RSS / 1024))
```

If RSS > 1500 MB or context feels heavy, request a restart:
```bash
gc agent request-restart
```
This sets `GC_RESTART_REQUESTED` metadata on the session and blocks
forever. The controller will kill and restart the session on the next
reconcile tick. The current wisp stays assigned and the new session
resumes it via `bd mol current`.

**2. Check mail:**
```bash
gc mail inbox
```

Handle each message type:

**HELP / Escalation:**
Assess the request. Can you help directly? If not, escalate:
```bash
gc mail send mayor/ -s "ESCALATION: <agent> needs help" -m "<details>"
```
Archive after handling.

**DOG_DONE:**
A utility agent completed its task (e.g., shutdown dance, dep
propagation). Note the outcome for observability. Archive.

**Other / informational:**
Archive after reading.

**Hygiene principle**: Archive messages after they're fully processed.
Inbox should be near-empty after this step.

Close this step and proceed."""

[[steps]]
id = "orphan-process-cleanup"
title = "Kill orphaned claude subagent processes"
needs = ["check-inbox"]
description = """
Claude Code's Task tool spawns subagent processes that sometimes don't
clean up. These accumulate and consume significant memory.

**Detection:** Orphaned subagent processes have TTY = "?" in `ps` output.
They are child processes of claude/node that lost their parent.

```bash
ps aux | grep -E 'claude|node' | grep '?'
```

**Judgment required:** Not all TTY=? processes are orphaned. Some are
legitimate background processes. Look for:
- Processes consuming significant memory (>500MB RSS)
- Processes that have been running much longer than any active session
- Multiple identical processes (accumulated from repeated crashes)

**Action:** Kill confirmed orphans:
```bash
kill <pid>
```

Use judgment — this is exactly why an LLM does it, not Go code.

Close this step after check (even if no orphans found)."""

[[steps]]
id = "check-gates"
title = "Evaluate and close pending gates"
needs = ["orphan-process-cleanup"]
description = """
Gates are town-wide coordination primitives. The deacon is the sole
evaluator — witnesses do not check gates.

**Step 1: Check timer gates:**
```bash
bd gate check --type=timer --escalate
```

This command:
1. Finds all open gate beads with await_type=timer
2. Checks if `now > created_at + timeout`
3. Closes elapsed gates
4. Escalates expired gates via mail

**Step 2: Check condition gates (if any):**

For gates with await_type=condition, evaluate the condition:
```bash
bd gate list --json
```
For each condition gate, run the check command and close if satisfied.

**Step 3: GitHub gates (if any):**

For gates with await_type=gh:run or gh:pr, check GitHub status and
close if the workflow passed or PR merged.

**Scheduling note:** When a gate closes, agents that hit the gated step
should have added a `blocks` dep from their work bead to the gate. The
dep is now satisfied and `bd ready` finds the work bead. Normal
scheduling handles re-dispatch — no deacon intervention needed.

Close this step after all gates evaluated."""

[[steps]]
id = "cross-rig-deps"
title = "Resolve cross-rig dependencies"
needs = ["check-gates"]
description = """
When an issue in one rig closes, dependent issues in other rigs may
still be blocked because `computeBlockedIDs` doesn't resolve external
deps across rig boundaries.

**Workaround:** The deacon converts satisfied cross-rig `blocks` deps
to `related` deps, preserving the audit trail while removing the
blocking semantics.

**Step 1: Find recently closed issues:**
```bash
bd list --status=closed --closed-after=<last-patrol-time> --json
```

Use the timestamp of the previous patrol cycle. If unknown (first cycle),
use a reasonable lookback (e.g., 1 hour).

**Step 2: For each closed issue, check for cross-rig dependents:**
```bash
bd dep list <id> --direction=up --type=blocks --json
```

Filter for external deps (prefixed with `external:`).

**Step 3: For each satisfied cross-rig dep, convert blocks → related:**
```bash
bd dep remove <dependent-id> external:<project>:<closed-issue>
bd dep add <dependent-id> external:<project>:<closed-issue> --type=related
```

This preserves the relationship for auditing while removing the blocking
semantics. `bd ready` in the dependent rig will now see the unblocked
work. No witness notification needed — normal scheduling handles it.

**Future:** When beads supports cross-rig `computeBlockedIDs`, this step
becomes unnecessary (see docs/bd-roadmap.md).

Close this step after check (even if no cross-rig deps found)."""

[[steps]]
id = "health-scan"
title = "Check work-layer health"
needs = ["cross-rig-deps"]
description = """
Monitor whether work is flowing through rig coordination agents.
The controller handles "is the agent running?" The deacon handles
"is work progressing?"

**Skip docked/parked rigs.** Only check active rigs.

**For each active rig, assess witness health:**

Check witness patrol wisp freshness. Each patrol cycle burns a wisp.
If the last wisp is much older than the maximum backoff cap (300s) plus
buffer, the witness may be stuck. But if there's no active work in the
rig, the witness is legitimately idle — not stuck.

**For each active rig, assess refinery health:**

Check refinery patrol wisp freshness + queue state:
- Wisp recently burned → healthy (actively cycling)
- Wisp open, no work assigned to refinery → idle, fine
- Wisp open, work assigned to refinery, stale `UpdatedAt` → stuck

**No hardcoded thresholds.** Read wisp timestamps, queue state, and
the nature of the current work. Make a judgment call about whether
something is stuck. This is exactly why an LLM does it, not Go code.

**For stuck coordination agents, file a warrant:**
```bash
bd create --type=warrant \
  --title="Stuck: <rig>/<role>" \
  --metadata '{"target":"<session>","reason":"<reason>","requester":"deacon"}' \
  --label=pool:dog
```

The dog pool runs `mol-shutdown-dance` for due process.

**Escalate to mayor** only for systemic issues (multiple rigs affected,
patterns of failure).

Close this step after assessment."""

[[steps]]
id = "periodic-formulas"
title = "Dispatch registered maintenance formulas"
needs = ["health-scan"]
description = """
Check if any registered maintenance formulas are due for execution.

Maintenance formulas are configured in `city.toml` under the deacon's
agent config. Each has a gate condition (cooldown, cron, condition, etc.)
that determines when it should fire.

**Step 1: Load registered formulas from config.**

**Step 2: For each formula, check gate condition:**

- **Cooldown:** Has enough time passed since the last run?
  Check plugin run history via bead labels.
- **Cron:** Does the current time match the schedule?
- **Condition:** Does the check command return exit 0?
- **Event:** Has the triggering event occurred?

**Step 3: For due formulas, pour a wisp and label for the target pool:**
```bash
WISP=$(bd mol wisp <formula-name> --json | jq -r '.new_epic_id')
bd update "$WISP" --label=<target-pool-labels>
```

**Step 4: Record the run** for cooldown gate tracking:
```bash
bd create --ephemeral --type=task \
  --title="Plugin run: <formula>" \
  --label=type:plugin-run,plugin:<name>,result:dispatched
```

Close this step after check (even if no formulas are due)."""

[[steps]]
id = "utility-agent-health"
title = "Check utility agent (dog) health"
needs = ["periodic-formulas"]
description = """
Detect utility agents (dogs) that are alive but stuck on their work.

The controller detects dead agents. But "working too long" is a
work-layer judgment: is this task genuinely slow, or is the agent stuck?
The controller can't know this. The deacon can, by checking bead and
wisp timestamps.

**Step 1: Find active utility agent work:**
```bash
bd list --status=in_progress --label=pool:dog --json --limit=0
```

**Step 2: For each, assess progress:**

Check the work bead's `UpdatedAt` and the agent's wisp freshness.
Consider the nature of the work — a shutdown dance with 240s timeouts
will naturally take longer than a quick dep propagation.

No hardcoded thresholds. Use judgment.

**Step 3: For stuck utility agents, file a warrant:**
```bash
bd create --type=warrant \
  --title="Stuck dog: <agent>" \
  --metadata '{"target":"<session>","reason":"<reason>","requester":"deacon"}' \
  --label=pool:dog
```

A different dog from the pool picks up the warrant and runs the
shutdown dance. If the pool is at capacity with all dogs stuck,
escalate to mayor.

Close this step after assessment."""

[[steps]]
id = "town-orphan-sweep"
title = "Town-wide orphan detection"
needs = ["utility-agent-health"]
description = """
Lightweight town-wide sanity check for orphaned work.

The per-rig witness handles detailed orphaned bead recovery (with
worktree salvage). The deacon does a broad sweep for things the
witnesses might miss:

- Beads assigned to agents that don't exist in ANY rig
- Beads assigned to deacon's own utility agents that died
- Cross-rig orphans (beads in one rig assigned to agents in another)

**Step 1: Find all in-progress beads:**
```bash
bd list --status=in_progress --json --limit=0
```

**Step 2: Cross-reference against all known agents:**
```bash
gc agent list --json
```

**Step 3: For obviously orphaned beads (agent doesn't exist anywhere):**
```bash
bd update <bead> --status=open --assignee=""
```

**Do NOT** attempt worktree salvage — that's the witness's job.
The deacon just resets the bead to the pool. The rig's witness will
pick it up on its next patrol and handle worktree recovery.

Close this step after sweep."""

[[steps]]
id = "system-health"
title = "Run system diagnostics"
needs = ["town-orphan-sweep"]
description = """
Run `gc doctor` for a quick system health check.

```bash
gc doctor
```

**For simple findings:** Act directly (e.g., stale lock files, temp
directory cleanup).

**For complex findings:** Escalate to mayor with context:
```bash
gc mail send mayor/ -s "DOCTOR: <finding>" -m "<details>"
```

Close this step after check."""

[[steps]]
id = "wisp-compact"
title = "Compact expired wisps"
needs = ["system-health"]
description = """
Wisps (ephemeral molecules) accumulate over time. Clean up expired ones.

**Step 1: Find closed wisps past their TTL:**
```bash
bd mol wisp gc --age 24h --dry-run
```

Review what would be cleaned up.

**Step 2: Execute compaction:**
```bash
bd mol wisp gc --age 24h
```

This deletes closed wisp beads and their child step beads that are
older than the TTL. Active (non-closed) wisps are never touched.

Close this step after compaction."""

[[steps]]
id = "next-iteration"
title = "Pour next iteration and loop"
needs = ["wisp-compact"]
description = """
**Config: event_timeout = {{event_timeout}}**

End of patrol cycle. Pour the next iteration, then wait or exit.

**1. Context check:**

If context feels heavy or RSS is high:
```bash
gc agent request-restart
```
This blocks forever. The controller restarts you. The next wisp
is already assigned — the new session resumes via `bd mol current`.

**2. Quick inbox check (end-of-cycle hygiene):**
```bash
gc mail inbox
```
Handle any urgent messages that arrived during patrol. Archive the rest.

**3. Pour next iteration BEFORE burning:**
```bash
NEXT=$(bd mol wisp mol-deacon-patrol --json | jq -r '.new_epic_id')
bd update "$NEXT" --assignee=$GC_AGENT
```

**4. Wait for activity (exponential backoff):**
```bash
SEQ=$(gc events --seq)
gc events --watch --type=bead.updated \
  --after=$SEQ --timeout {{event_timeout}}s
```

On event: proceed immediately.
On timeout: double the timeout (cap 300s) and proceed anyway.

**5. Burn this wisp:**
```bash
bd mol burn <this-wisp-id> --force
```

The new wisp is ready. `bd mol current` will find it and start
the next patrol cycle.

**Exit criteria:** Next wisp poured, this wisp burned."""
