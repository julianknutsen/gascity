description = """
Deacon patrol loop. Poured as a wisp on startup:

  bd mol wisp mol-deacon-patrol
  bd update $WISP --assignee=$GC_AGENT

Each wisp is ONE iteration: check inbox, run town-wide coordination
tasks, pour the next iteration. On crash, `bd mol current` shows
exactly where you left off.

The loop mechanism: every exit path (happy or early) pours the next
wisp before burning this one. The prompt only bootstraps the first wisp.

## Deacon Role

The deacon is the **LLM sidekick to the controller**. It handles periodic
tasks that require judgment or observation — things the Go controller
can't or shouldn't do.

1. **Work-layer health** — are witnesses and refineries making progress?
   (Not "are they running" — that's the controller's job.)
2. **Utility agent health** — detect stuck dogs, dispatch shutdown dance.
3. **Infrastructure cleanup** — orphaned processes, expired wisps,
   system diagnostics.

Mechanical tasks (gate evaluation, cross-rig deps, orphan sweeps) are
handled by exec automations in the maintenance topology — no LLM needed.

## Idle Town Principle

The deacon should be silent/invisible when the town is healthy and idle.
Skip health checks when no active work exists. Use exponential backoff
between patrol cycles.

## What the deacon does NOT do

- Start/stop/restart agents (controller handles this)
- Per-rig orphaned bead recovery (witness handles this)
- Code implementation (polecats do this)
- Kill agents directly (files warrants, dog pool runs shutdown dance)
- Pool sizing (controller pool reconciliation)

Read each step's description before acting — Config values override defaults."""
formula = "mol-deacon-patrol"
version = 12

[vars]
[vars.event_timeout]
description = "Seconds to wait for events before re-checking (exponential backoff)"
default = "30"

[[steps]]
id = "check-inbox"
title = "Context check, then mail"
description = """
**1. Context check (FIRST — before any work):**
```bash
RSS=$(ps -o rss= -p $$ | tr -d ' ')
RSS_MB=$((RSS / 1024))
```

If RSS > 1500 MB or context feels heavy, request a restart:
```bash
gc agent request-restart
```
This sets `GC_RESTART_REQUESTED` metadata on the session and blocks
forever. The controller will kill and restart the session on the next
reconcile tick. The current wisp stays assigned and the new session
resumes it via `bd mol current`.

**2. Check mail:**
```bash
gc mail inbox
```

Handle each message type:

**HELP / Escalation:**
Assess the request. Can you help directly? If not, escalate:
```bash
gc mail send mayor/ -s "ESCALATION: <agent> needs help" -m "<details>"
```
Archive after handling.

**DOG_DONE:**
A utility agent completed its task (e.g., shutdown dance, dep
propagation). Note the outcome for observability. Archive.

**Other / informational:**
Archive after reading.

**Hygiene principle**: Archive messages after they're fully processed.
Inbox should be near-empty after this step.

Close this step and proceed."""

[[steps]]
id = "orphan-process-cleanup"
title = "Kill orphaned claude subagent processes"
needs = ["check-inbox"]
description = """
Claude Code's Task tool spawns subagent processes that sometimes don't
clean up. These accumulate and consume significant memory.

**Detection:** Orphaned subagent processes have TTY = "?" in `ps` output.
They are child processes of claude/node that lost their parent.

```bash
ps aux | grep -E 'claude|node' | grep '?'
```

**Judgment required:** Not all TTY=? processes are orphaned. Some are
legitimate background processes. Look for:
- Processes consuming significant memory (>500MB RSS)
- Processes that have been running much longer than any active session
- Multiple identical processes (accumulated from repeated crashes)

**Action:** Kill confirmed orphans:
```bash
kill <pid>
```

Use judgment — this is exactly why an LLM does it, not Go code.

Close this step after check (even if no orphans found)."""

[[steps]]
id = "health-scan"
title = "Check work-layer health"
needs = ["orphan-process-cleanup"]
description = """
Monitor whether work is flowing through rig coordination agents.
The controller handles "is the agent running?" The deacon handles
"is work progressing?"

**Skip docked/parked rigs.** Only check active rigs.

**For each active rig, assess witness health:**

Check witness patrol wisp freshness. Each patrol cycle burns a wisp.
If the last wisp is much older than the maximum backoff cap (300s) plus
buffer, the witness may be stuck. But if there's no active work in the
rig, the witness is legitimately idle — not stuck.

**For each active rig, assess refinery health:**

Check refinery patrol wisp freshness + queue state:
- Wisp recently burned → healthy (actively cycling)
- Wisp open, no work assigned to refinery → idle, fine
- Wisp open, work assigned to refinery, stale `UpdatedAt` → stuck

**No hardcoded thresholds.** Read wisp timestamps, queue state, and
the nature of the current work. Make a judgment call about whether
something is stuck. This is exactly why an LLM does it, not Go code.

**For stuck coordination agents, file a warrant:**
```bash
bd create --type=warrant \
  --title="Stuck: <rig>/<role>" \
  --metadata '{"target":"<session>","reason":"<reason>","requester":"deacon"}' \
  --label=pool:dog
```

The dog pool runs `mol-shutdown-dance` for due process.

**Escalate to mayor** only for systemic issues (multiple rigs affected,
patterns of failure).

Close this step after assessment."""

[[steps]]
id = "utility-agent-health"
title = "Check utility agent (dog) health"
needs = ["health-scan"]
description = """
Detect utility agents (dogs) that are alive but stuck on their work.

The controller detects dead agents. But "working too long" is a
work-layer judgment: is this task genuinely slow, or is the agent stuck?
The controller can't know this. The deacon can, by checking bead and
wisp timestamps.

**Step 1: Find active utility agent work:**
```bash
bd list --status=in_progress --label=pool:dog --json --limit=0
```

**Step 2: For each, assess progress:**

Check the work bead's `UpdatedAt` and the agent's wisp freshness.
Consider the nature of the work — a shutdown dance with 240s timeouts
will naturally take longer than a quick dep propagation.

No hardcoded thresholds. Use judgment.

**Step 3: For stuck utility agents, file a warrant:**
```bash
bd create --type=warrant \
  --title="Stuck dog: <agent>" \
  --metadata '{"target":"<session>","reason":"<reason>","requester":"deacon"}' \
  --label=pool:dog
```

A different dog from the pool picks up the warrant and runs the
shutdown dance. If the pool is at capacity with all dogs stuck,
escalate to mayor.

Close this step after assessment."""

[[steps]]
id = "system-health"
title = "Run system diagnostics"
needs = ["utility-agent-health"]
description = """
Run `gc doctor` for a quick system health check.

```bash
gc doctor
```

**For simple findings:** Act directly (e.g., stale lock files, temp
directory cleanup).

**For complex findings:** Escalate to mayor with context:
```bash
gc mail send mayor/ -s "DOCTOR: <finding>" -m "<details>"
```

Close this step after check."""

[[steps]]
id = "wisp-compact"
title = "Compact expired wisps"
needs = ["system-health"]
description = """
Wisps (ephemeral molecules) accumulate over time. Clean up expired ones.

**Step 1: Find closed wisps past their TTL:**
```bash
bd mol wisp gc --age 24h --dry-run
```

Review what would be cleaned up.

**Step 2: Execute compaction:**
```bash
bd mol wisp gc --age 24h
```

This deletes closed wisp beads and their child step beads that are
older than the TTL. Active (non-closed) wisps are never touched.

Close this step after compaction."""

[[steps]]
id = "next-iteration"
title = "Pour next iteration and loop"
needs = ["wisp-compact"]
description = """
**Config: event_timeout = {{event_timeout}}**

End of patrol cycle. Pour the next iteration, then wait or exit.

**1. Context check:**

If context feels heavy or RSS is high:
```bash
gc agent request-restart
```
This blocks forever. The controller restarts you. The next wisp
is already assigned — the new session resumes via `bd mol current`.

**2. Quick inbox check (end-of-cycle hygiene):**
```bash
gc mail inbox
```
Handle any urgent messages that arrived during patrol. Archive the rest.

**3. Pour next iteration BEFORE burning:**
```bash
NEXT=$(bd mol wisp mol-deacon-patrol --json | jq -r '.new_epic_id')
bd update "$NEXT" --assignee=$GC_AGENT
```

**4. Wait for activity (exponential backoff):**
```bash
SEQ=$(gc events --seq)
gc events --watch --type=bead.updated \
  --after=$SEQ --timeout {{event_timeout}}s
```

On event: proceed immediately.
On timeout: double the timeout (cap 300s) and proceed anyway.

**5. Burn this wisp:**
```bash
bd mol burn <this-wisp-id> --force
```

The new wisp is ready. `bd mol current` will find it and start
the next patrol cycle.

**Exit criteria:** Next wisp poured, this wisp burned."""
