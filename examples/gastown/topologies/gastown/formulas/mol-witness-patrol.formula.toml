description = """
Witness patrol loop. Poured as a wisp on startup:

  bd mol wisp mol-witness-patrol
  bd update $WISP --assignee=$GC_AGENT

Each wisp is ONE iteration: check for work, patrol, pour the next
iteration. On crash, `bd mol current` shows exactly where you left off.

The loop mechanism: every exit path (happy or early) pours the next
wisp before burning this one. The prompt only bootstraps the first wisp.

## Witness Role

The witness is the rig's work-health monitor. It does NOT manage processes
(the controller handles start/stop/restart/zombie detection). The witness
monitors the WORK layer:

1. **Orphaned bead recovery** — beads assigned to agents that won't spawn
   (pool max changed, agent removed from config). This is the core job.
2. **Refinery queue health** — work beads assigned to refinery, staleness.
3. **Polecat health** — detect stuck polecats, file warrants for dog pool.
4. **Help mail** — triage HELP/escalation requests from polecats.

Gate checks and convoy/swarm completion are town-wide concerns handled by
the deacon, not the per-rig witness.

## Canonical Work Chain

```
worktree → (push) → branch → (merge) → target branch
```

Each transition moves the canonical location of the work. Once moved,
the previous location is disposable:
- After push: worktree disposable (branch is canonical)
- After merge: branch disposable (target is canonical)

The witness's core recovery job: when a bead is orphaned (agent won't
come back), ensure the work reaches the branch (push), then clean up
the worktree. This makes the work schedulable again.

## What the witness does NOT do

- Zombie detection (controller reconcile loop handles this)
- Process start/stop (controller handles this)
- Code implementation (polecats do this)
- Gate checks (deacon handles town-wide)
- Convoy/swarm completion (deacon handles cross-rig)
- Kill stuck agents directly (files warrant, dog pool runs shutdown dance)

Read each step's description before acting — Config values override defaults."""
formula = "mol-witness-patrol"
version = 7

[vars]
[vars.event_timeout]
description = "Seconds to wait for events before re-checking (exponential backoff)"
default = "30"

[[steps]]
id = "check-inbox"
title = "Context check, then mail"
description = """
**1. Context check (FIRST — before any work):**
```bash
RSS=$(ps -o rss= -p $$ | tr -d ' ')
RSS_MB=$((RSS / 1024))
```

If RSS > 1500 MB or context feels heavy, request a restart:
```bash
gc agent request-restart
```
This sets `GC_RESTART_REQUESTED` metadata on the session and blocks
forever. The controller will kill and restart the session on the next
reconcile tick. The current wisp stays assigned and the new session
resumes it via `bd mol current`.

**2. Check mail:**
```bash
gc mail inbox
```

Handle each message type:

**HELP / Blocked:**
Assess the request. Can you help (point to docs, suggest approach)?
If not, escalate:
```bash
gc mail send mayor/ -s "ESCALATION: <polecat> needs help" -m "<details>"
```
Archive after handling.

**HANDOFF:**
Read predecessor context. Continue from where they left off.
Archive after absorbing context.

**Other / informational:**
Archive after reading.

**Hygiene principle**: Archive messages after they're fully processed.
Inbox should be near-empty after this step.

Close this step and proceed."""

[[steps]]
id = "recover-orphaned-beads"
title = "Recover orphaned work beads"
needs = ["check-inbox"]
description = """
**This is the witness's core job.**

Find beads assigned to agents that will never process them, salvage any
unpublished work from the worktree, and return beads to the pool.

This happens when:
- Pool max was reduced (agents that won't spawn anymore)
- An agent was removed from config
- An agent crashed and the controller decided not to restart it (quarantine)

**Step 1: Find orphaned beads.**

List beads assigned to agents in YOUR rig:
```bash
bd list --status=in_progress --json --limit=0
bd list --status=open --json --limit=0
```

Filter for beads assigned to polecat-pattern agents (e.g., `<rig>/polecats/<name>`).

Cross-reference against running and desired agents:
```bash
gc agent list --json
```

For each bead with a polecat assignee:
- If the assigned agent is running → not orphaned, skip
- If the assigned agent is NOT running but IS a desired agent
  (appears in agent config) → controller will restart it, skip
- If the agent is neither running nor desired → **orphaned bead**

**Important**: Do NOT recover beads assigned to agents that are simply
restarting (crash recovery). The controller restarts crashed agents,
and the fresh session resumes work via `bd mol current`. Only recover
beads when the agent genuinely won't come back.

**Step 2: For each orphaned bead, salvage work from the worktree.**

The canonical work chain: `worktree → (push) → branch → (merge) → target`.
Our job is to ensure work reaches the branch so it's schedulable.

Read the bead metadata to find the worktree and branch:
```bash
META=$(bd show <bead> --json | jq '.metadata')
WORKTREE=$(echo "$META" | jq -r '.worktree // empty')
BRANCH=$(echo "$META" | jq -r '.branch // empty')
```

Check the bead and worktree state, then act:

**Case A: Branch exists on origin (work already published).**
Worktree is disposable — canonical work is on the branch.
```bash
git ls-remote --heads origin $BRANCH  # Verify branch exists on remote
```
If branch exists on origin → skip to Step 3 (cleanup).

**Case B: Worktree exists, branch pushed but not on origin yet.**
Branch is recorded in metadata but polecat may have crashed before push.
```bash
cd "$WORKTREE"
git ls-remote --heads origin $BRANCH
```
If branch IS on origin → skip to Step 3.
If branch is NOT on origin → fall through to Case C.

**Case C: Worktree exists with unpushed commits.**
Work is committed locally but never pushed. Branch is not yet canonical.
```bash
cd "$WORKTREE"
BRANCH=$(git branch --show-current)
git log origin/main..HEAD --oneline  # Shows unpushed commits
```
Commit any remaining uncommitted/untracked work first (safeguard):
```bash
git status --porcelain
# If any output:
git add -A
git commit -m "witness: salvage uncommitted work (<bead>)"
```
Publish the work to make branch canonical:
```bash
git push origin HEAD
bd update <bead> --set-metadata branch=$BRANCH
```
Skip to Step 3 (cleanup).

**Case D: Worktree exists with ONLY uncommitted/untracked changes (no commits).**
Same resolution as Case C. All work is useful work — never discard.
```bash
cd "$WORKTREE"
git add -A
git commit -m "witness: salvage work from orphaned agent (<bead>)"
BRANCH=$(git branch --show-current)
git push origin HEAD
bd update <bead> --set-metadata branch=$BRANCH
```
Skip to Step 3 (cleanup).

**Case E: No worktree exists (`metadata.worktree` missing or directory gone),
no branch on origin.**
Nothing to salvage. The bead goes back to pool and a new polecat
starts fresh.

**Step 3: Clean up worktree and return bead to pool.**

Once the branch is canonical (on origin), the worktree is disposable:
```bash
# Delete worktree if it exists
cd <rig-root>
git worktree remove <worktree-path> --force 2>/dev/null
rm -rf <worktree-path> 2>/dev/null  # Fallback
git worktree prune
```

Reset bead to pool:
```bash
bd update <bead> --status=open --assignee=""
```

**Step 4: Assess and notify.**

Always log each recovery as an event. Mail the mayor only when the
recovery is unexpected or concerning — use your judgment:

| Situation | Action |
|-----------|--------|
| Pool resize / config change removed agent | Log only. Routine. |
| Agent crashed mid-work, branch already on origin | Log + nudge mayor. Low urgency. |
| Work salvaged from worktree (data was at risk) | Log + mail mayor. Work was nearly lost. |
| Same bead recovered before (check `metadata.recovered`) | Log + mail mayor. Possible crash loop. |

When you do mail:
```bash
gc mail send mayor/ -s "ORPHAN_RECOVERED: <bead>" \
  -m "Bead <bead> was assigned to <agent> which is no longer active.
Recovery: <what was done — branch pushed / metadata set / work salvaged / nothing to salvage>
Branch: <branch name or 'none'>
Status: Reset to open pool for re-dispatch.
Concern: <why this one warrants attention>"
```

Mark recovered beads so spawn storm detection can track patterns:
```bash
bd update <bead> --set-metadata recovered=true
```

**Exit criteria:** All orphaned beads recovered, worktrees cleaned.
Or no orphans found."""

[[steps]]
id = "check-refinery"
title = "Check refinery queue health"
needs = ["recover-orphaned-beads"]
description = """
Ensure the refinery is processing work and the queue is healthy.

**Step 1: Check work beads assigned to refinery:**
```bash
bd list --assignee=<rig>/refinery --status=open --json
```

These are work beads with `metadata.branch` and `metadata.target` that
polecats have submitted for merging. Look for:

- **Stale beads**: Assigned to refinery but old `UpdatedAt`. The refinery
  may be stuck or the bead may have been missed.
- **Queue depth**: Many beads waiting may indicate the refinery is
  overwhelmed or down.
- **Empty queue**: No work assigned — refinery is idle, which is fine.

**Step 2: Assess refinery wisp freshness:**

Check when the refinery last completed a patrol cycle by looking at
recent burned wisps. If the refinery has work assigned but its last
wisp is stale, the refinery may be stuck.

**Step 3: Nudge if needed:**
```bash
gc nudge <rig>/refinery "Work beads waiting for merge. Please check queue."
```

**Step 4: Escalate if needed:**
```bash
gc mail send mayor/ -s "QUEUE_HEALTH: <summary>" \
  -m "Work bead IDs: <ids>
Observation: <what you found>
Recommendation: <what should happen>"
```

Close this step after assessment."""

[[steps]]
id = "check-polecat-health"
title = "Check polecat work progress"
needs = ["check-refinery"]
description = """
Detect polecats that are alive but not making progress on their work.

The controller detects dead agents and restarts them. But a polecat can
be alive and stuck — infinite loop, blocked on something, or just not
progressing. The controller can't detect this; it's a work-layer judgment.

**Step 1: Find active polecat work beads:**
```bash
bd list --status=in_progress --json --limit=0
```
Filter for beads assigned to polecat-pattern agents in YOUR rig.

**Step 2: For each, assess progress:**

Check the work bead's `UpdatedAt` timestamp and the polecat's molecule
wisp freshness. Consider:

- **Recently updated** → making progress, skip
- **Stale but agent is in a long tool call** → might be fine, use judgment
- **Very stale, no wisp progress** → likely stuck

There are no hardcoded thresholds. Consider the nature of the work, the
time elapsed, and whether the agent shows any signs of activity. This is
judgment work — exactly why an LLM does it, not Go code.

**Step 3: For stuck polecats, file a warrant:**

Do NOT kill the agent directly. File a warrant bead and let the dog pool
handle the shutdown dance (multi-stage interrogation with due process):

```bash
bd create --type=warrant \
  --title="Stuck: <agent>" \
  --metadata '{"target":"<session-name>","reason":"No progress on <bead> for <duration>","requester":"witness"}' \
  --label=pool:dog
```

The dog pool picks up the warrant and runs `mol-shutdown-dance`, which
gives the polecat 3 chances to prove it's alive before killing it.

**Step 4: Log assessment:**

Note which polecats were checked and their status for observability.
Don't escalate to mayor for individual stuck polecats — the warrant
system handles it. Only escalate if you see a pattern (many stuck
polecats, systemic issue).

**Exit criteria:** All active polecat work beads assessed. Warrants
filed for stuck agents. Or no stuck polecats found."""

[[steps]]
id = "next-iteration"
title = "Pour next iteration and loop"
needs = ["check-polecat-health"]
description = """
**Config: event_timeout = {{event_timeout}}**

End of patrol cycle. Pour the next iteration, then wait or exit.

**1. Context check:**

If context feels heavy or RSS is high:
```bash
gc agent request-restart
```
This blocks forever. The controller restarts you. The next wisp
is already assigned — the new session resumes via `bd mol current`.

**2. Pour next iteration BEFORE burning:**
```bash
NEXT=$(bd mol wisp mol-witness-patrol --json | jq -r '.new_epic_id')
bd update "$NEXT" --assignee=$GC_AGENT
```

**3. Wait for activity (exponential backoff):**
```bash
SEQ=$(gc events --seq)
gc events --watch --type=bead.updated \
  --after=$SEQ --timeout {{event_timeout}}s
```

On event: proceed immediately.
On timeout: double the timeout (cap 300s) and proceed anyway.

**4. Burn this wisp:**
```bash
bd mol burn <this-wisp-id> --force
```

The new wisp is ready. `bd mol current` will find it and start
the next patrol cycle.

**Exit criteria:** Next wisp poured, this wisp burned."""
